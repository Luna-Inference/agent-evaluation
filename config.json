{
  "categories": [
    "general knowledge",
    "science",
    "math",
    "history",
    "creative writing",
    "coding",
    "geography",
    "literature",
    "philosophy",
    "linguistics",
    "art",
    "music",
    "sports",
    "technology",
    "business",
    "politics",
    "psychology",
    "physics",
    "chemistry",
    "biology"
  ],
  "evaluation_criteria": [
    "Correctness: Is the answer truthfully correct?",
    "Number of steps: How many steps were taken to arrive at the final answer?",
    "Failure count: How many times did code execution fail?",
    "Misunderstanding count: How many times did the LLM use a tool incorrectly?",
    "Token usage: How many prefill and generation tokens were used?",
    "Time: What was the total end-to-end time taken?",
    "Self-confidence: How confident is the model in its own internal knowledge?"
  ],
  "question_types": {
    "fact_retrieval": "Simple factual questions where the agent should know or retrieve the answer.",
    "reasoning": "Questions requiring logical reasoning or calculation.",
    "tool_usage": "Tasks requiring specific tools like calculators, code execution, etc.",
    "multi_step_reasoning": "Problems requiring breaking down into multiple steps.",
    "creative_generation": "Tasks requiring original content creation."
  }
}